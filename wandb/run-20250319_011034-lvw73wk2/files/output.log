tokens per iteration will be: 491,520
Initializing a new model from scratch
Initializing a new model from scratch
number of parameters: 120.01M
/users/4/serra082/gpt2mla/train_mla.py:184: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
using fused AdamW: True
step 0: train loss 10.9600, val loss 10.9594
iter 0: loss 10.9531, time 12250.62ms, mfu -100.00%
iter 10: loss 10.4109, time 2880.88ms, mfu 22.78%
iter 20: loss 9.7674, time 2608.85ms, mfu 23.02%
iter 30: loss 9.2870, time 2739.39ms, mfu 23.12%
iter 40: loss 9.1441, time 2576.73ms, mfu 23.35%
iter 50: loss 9.1684, time 2594.29ms, mfu 23.55%
iter 60: loss 8.7424, time 2660.87ms, mfu 23.66%
iter 70: loss 8.4222, time 2617.79ms, mfu 23.80%
iter 80: loss 8.3856, time 2920.71ms, mfu 23.67%
iter 90: loss 8.3303, time 2650.71ms, mfu 23.78%
iter 100: loss 7.7640, time 2695.04ms, mfu 23.84%
iter 110: loss 7.6220, time 2591.26ms, mfu 23.98%
iter 120: loss 7.6433, time 2571.44ms, mfu 24.14%
iter 130: loss 7.5406, time 2918.33ms, mfu 23.97%
iter 140: loss 7.1071, time 2614.92ms, mfu 24.09%
iter 150: loss 7.2697, time 2595.05ms, mfu 24.21%
iter 160: loss 6.9832, time 2591.75ms, mfu 24.32%
iter 170: loss 6.7994, time 2629.91ms, mfu 24.38%
iter 180: loss 6.8935, time 2577.81ms, mfu 24.49%
iter 190: loss 6.5674, time 2593.69ms, mfu 24.57%
iter 200: loss 6.2186, time 2632.42ms, mfu 24.61%
iter 210: loss 6.3176, time 2577.96ms, mfu 24.69%
iter 220: loss 6.2831, time 2577.26ms, mfu 24.77%
iter 230: loss 6.6230, time 2671.47ms, mfu 24.75%
iter 240: loss 5.8439, time 2590.17ms, mfu 24.81%
iter 250: loss 6.2645, time 2894.73ms, mfu 24.60%
iter 260: loss 6.0951, time 3081.20ms, mfu 24.27%
iter 270: loss 6.2886, time 2684.44ms, mfu 24.29%
iter 280: loss 6.6033, time 2577.65ms, mfu 24.40%
